{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "only use the 1000 most frequent words\n"
     ]
    }
   ],
   "source": [
    "print('only use the 1000 most frequent words')\n",
    "\n",
    "TOP_WORDS=1000 # only use the 1000 most frequent words\n",
    "INDEX_FROM=3   # word index offset\n",
    "\n",
    "train,test = imdb.load_data(num_words=TOP_WORDS, index_from=INDEX_FROM)\n",
    "X_train,y_train = train\n",
    "X_test,y_test = test\n",
    "\n",
    "word_to_id = imdb.get_word_index()\n",
    "word_to_id = {k:(v+INDEX_FROM) for k,v in word_to_id.items()}\n",
    "word_to_id[\"<PAD>\"] = 0\n",
    "word_to_id[\"<START>\"] = 1\n",
    "word_to_id[\"<UNK>\"] = 2\n",
    "id_to_word = {value:key for key,value in word_to_id.items()}\n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "examples on mapping word and id\n",
      "id-----word: 4 -----the\n",
      "word-----id: casting -----973\n"
     ]
    }
   ],
   "source": [
    "print('examples on mapping word and id')\n",
    "print('id-----word: 4 -----'+id_to_word[4])\n",
    "print('word-----id: casting -----'+str(word_to_id['casting']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_review_length = 128\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 128, 32)           32000     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               82432     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 114,561\n",
      "Trainable params: 114,561\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/2\n",
      "20000/20000 [==============================] - 24s 1ms/step - loss: 0.6153 - acc: 0.6368 - val_loss: 0.4212 - val_acc: 0.8130\n",
      "Epoch 2/2\n",
      "20000/20000 [==============================] - 24s 1ms/step - loss: 0.3932 - acc: 0.8288 - val_loss: 0.3929 - val_acc: 0.8360\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12d5802b0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the model\n",
    "embedding_vecor_length = 32\n",
    "num_LSTM = 128\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(TOP_WORDS, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(LSTM(num_LSTM))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, epochs=2, batch_size=256,\n",
    "         validation_split = 0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: Loss over the test dataset: 0.38, Accuracy: 0.84\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Summary: Loss over the test dataset: %.2f, Accuracy: %.2f' % (score[0], score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printing_lstm_pred_prob(X):\n",
    "    positive_prob = model.predict(np.reshape(X, (1, -1)), batch_size=1)\n",
    "    print('-------- LSTM prediction: probability of being positive:  '+str(np.round(positive_prob[0,0]*100))+'%')\n",
    "def printing_id2word(X):\n",
    "    print(' '.join(id_to_word[id] for id in X))\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment analysis on IMDB movie reviews\n",
      "*\n",
      "<PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <START> please give this one a miss br br <UNK> <UNK> and the rest of the cast <UNK> terrible performances the show is <UNK> <UNK> <UNK> br br i don't know how michael <UNK> could have <UNK> this one on his <UNK> he almost seemed to know this wasn't going to work out and his performance was quite <UNK> so all you <UNK> fans give this a miss\n",
      "-------- LSTM prediction: probability of being positive:  55.0%\n",
      "-------- ground truth:0\n",
      "*\n",
      "young man <UNK> <UNK> <UNK> michael <UNK> has a small part the <UNK> <UNK> set <UNK> the <UNK> of the story very well in short this movie is a powerful <UNK> of <UNK> sexual <UNK> and <UNK> be <UNK> <UNK> up the atmosphere and pay attention to the <UNK> written script br br i <UNK> robert <UNK> this is one of his many films that <UNK> with <UNK> <UNK> subject matter this film is <UNK> but it's <UNK> and it's sure to <UNK> a strong emotional <UNK> from the viewer if you want to see an <UNK> film some might even say <UNK> this is worth the time br br unfortunately it's very difficult to find in video <UNK> you may have to buy it off the <UNK>\n",
      "-------- LSTM prediction: probability of being positive:  98.0%\n",
      "-------- ground truth:1\n",
      "*\n",
      "enough alone br br <UNK> the time period when this <UNK> little film was made and <UNK> the fact that it was made by a <UNK> <UNK> at the <UNK> of that <UNK> <UNK> <UNK> war it would be easy to see this as a <UNK> about those events <UNK> may or may not have had <UNK> <UNK> in mind when he made <UNK> but whatever <UNK> his <UNK> of material the film <UNK> as a <UNK> tale of <UNK> <UNK> <UNK> could be the <UNK> <UNK> <UNK> <UNK> or <UNK> in the <UNK> or any country of any era that <UNK> its <UNK> down and is <UNK> by <UNK> it's a <UNK> film even a <UNK> one in its <UNK> way but its message is no joke\n",
      "-------- LSTM prediction: probability of being positive:  66.0%\n",
      "-------- ground truth:1\n",
      "*\n",
      "am a <UNK> man pretty big and i can <UNK> myself well however i would not do half the stuff the little girl does in this movie also the mother in this movie is <UNK> with her children to the point of <UNK> i wish i wasn't so <UNK> about her and her <UNK> because i would have otherwise enjoyed the flick what a number she was take my <UNK> and fast forward through everything you see her do until the end also is anyone else getting <UNK> of watching movies that are filmed so dark <UNK> one can hardly see what is being filmed as an audience we are <UNK> involved with the <UNK> on the screen so then why the hell can't we have night <UNK>\n",
      "-------- LSTM prediction: probability of being positive:  42.0%\n",
      "-------- ground truth:0\n",
      "*\n",
      "<PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <START> like some other people <UNK> i'm a die hard <UNK> fan and i loved this game br br this game starts <UNK> boring but <UNK> me it's worth it as soon as you start your <UNK> the <UNK> are fun and <UNK> they will <UNK> you <UNK> your mind turns to <UNK> i'm not <UNK> this game is also <UNK> and is <UNK> done br br to keep this <UNK> free i have to keep my <UNK> <UNK> about <UNK> but please try this game <UNK> be worth it br br story 9 9 action 10 1 it's that good <UNK> 10 attention <UNK> 10 average 10\n",
      "-------- LSTM prediction: probability of being positive:  99.0%\n",
      "-------- ground truth:1\n",
      "*\n",
      "this movie isn't being <UNK> all who love this movie should <UNK> disney and <UNK> the <UNK> for it <UNK> eventually have to <UNK> it then i'd buy <UNK> for <UNK> i know everything and <UNK> in this movie did a good job and i haven't <UNK> out why disney <UNK> put this movie on dvd or on <UNK> in <UNK> <UNK> at least i haven't seen any <UNK> this is a <UNK> good movie and should be seen by all the kids in the new <UNK> don't get to see it and i think they should it should at least be put back on the <UNK> this movie doesn't <UNK> a cheap <UNK> it <UNK> the real thing i'm them now this movie will be on dvd\n",
      "-------- LSTM prediction: probability of being positive:  75.0%\n",
      "-------- ground truth:1\n",
      "*\n",
      "a <UNK> <UNK> of the horror of the <UNK> <UNK> br br the death of a young mother leads to a baby <UNK> <UNK> down the <UNK> in a sequence that has been <UNK> by <UNK> in <UNK> <UNK> by <UNK> <UNK> in <UNK> and <UNK> <UNK> in the <UNK> this sequence is shown <UNK> from various <UNK> <UNK> <UNK> out what probably was only a five second <UNK> br br <UNK> is a film that the <UNK> <UNK> <UNK> it for those already <UNK> and it for the <UNK> it <UNK> of fire and <UNK> with the <UNK> <UNK> of the <UNK> <UNK> <UNK> its greatest <UNK> has been on film <UNK> who have <UNK> and only <UNK> <UNK> on <UNK> <UNK> in <UNK> several <UNK> ago\n",
      "-------- LSTM prediction: probability of being positive:  65.0%\n",
      "-------- ground truth:1\n",
      "*\n",
      "must <UNK> dog so that she can <UNK> her <UNK> <UNK> this is <UNK> and the <UNK> fall in love so do <UNK> and <UNK> the rest of the film <UNK> by with romance and at the end <UNK> dog gives <UNK> but who is the father br br the dog story is the very weak <UNK> that is used to try and create a story between <UNK> its a terrible storyline there are 3 main musical <UNK> all of which are <UNK> bad songs and <UNK> <UNK> its just an extremely boring film <UNK> has too many words in each <UNK> and <UNK> them in an almost <UNK> <UNK> its not funny ever but its meant to be <UNK> and <UNK> have done much better than this\n",
      "-------- LSTM prediction: probability of being positive:  8.0%\n",
      "-------- ground truth:0\n",
      "*\n",
      "<UNK> with <UNK> <UNK> <UNK> <UNK> but few of these <UNK> have <UNK> the <UNK> of time the most memorable were the <UNK> <UNK> <UNK> <UNK> films which have long since become <UNK> this one is <UNK> with <UNK> songs <UNK> <UNK> and <UNK> it's a truly <UNK> of <UNK> and pretty near <UNK> today it was <UNK> for its <UNK> special effects which are almost <UNK> in this day and age <UNK> <UNK> of <UNK> <UNK> the only <UNK> <UNK> feature which <UNK> is its beautiful <UNK> and <UNK> sad to say of the many films made in this genre few of them come up to <UNK> <UNK> original <UNK> of <UNK> almost any other <UNK> <UNK> film is <UNK> to this one though it's a <UNK>\n",
      "-------- LSTM prediction: probability of being positive:  93.0%\n",
      "-------- ground truth:0\n",
      "*\n",
      "<UNK> <UNK> <UNK> whom <UNK> <UNK> both as himself and as the <UNK> the four <UNK> turn in excellent performances especially <UNK> and <UNK> while together <UNK> and <UNK> <UNK> the <UNK> side of <UNK> <UNK> br br there are some <UNK> <UNK> in this film about the only <UNK> i can really point out is a certain to the script in some <UNK> which i think is due mostly to the way this film is a four <UNK> fight there simply isn't enough time to <UNK> <UNK> what's going on br br <UNK> this is a <UNK> good film i highly recommend watching this in <UNK> with the first and then <UNK> for how good the series could have been had it <UNK> under <UNK> and <UNK>\n",
      "-------- LSTM prediction: probability of being positive:  95.0%\n",
      "-------- ground truth:1\n"
     ]
    }
   ],
   "source": [
    "print('sentiment analysis on IMDB movie reviews')\n",
    "for i in range(10):\n",
    "    print('*')\n",
    "    printing_id2word(X_test[i])\n",
    "    printing_lstm_pred_prob(X_test[i])\n",
    "    print('-------- ground truth:'+str(y_test[i]) )    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
